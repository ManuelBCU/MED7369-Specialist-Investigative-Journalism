{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python and Morph.io (for scraping)\n",
    "\n",
    "This tutorial is intended to build on some basic coding concepts and introduce Morph.io. By the end you should:\n",
    "\n",
    "* Be able to use GitHub to edit Python files\n",
    "* Use Morph.io to run Python code hosted on GitHub\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "\n",
    "## Get started with Morph.io\n",
    "\n",
    "1. [create an account on GitHub](https://github.com/) if you haven't got one already, and [sign in to Morph.io using your GitHub account](https://morph.io/users/auth/github)\n",
    "2. [Click **New scraper**](https://morph.io/scrapers/new) on the menu at the top of Morph.io. You will be taken to a new page asking you to specify more details\n",
    "3. On the dropdown menu for *Language*, select **Python**. Give your scraper a name in the next box - something like 'startingtocode' (no spaces), and in the final box write 'none' - this isn't a scraper yet: we're just using Morph.io as a place to learn code.\n",
    "4. Click **Create scraper**\n",
    "\n",
    "It will take Morph.io a few moments to create the files for your scraper (the files are being created on GitHub). \n",
    "\n",
    "When it has finished, you will be taken to a new page for the scraper. Look on the right where it says *Scraper code*. There should be a link to `startingtocode / scraper.py ` - this will take you to the pages on GitHub where the code is now hosted: `scraper.py` is the file for the code itself; `startingtocode` is the link to the repository containing that file.\n",
    "\n",
    "Open the link to `scraper.py` in a separate tab or window, but also keep your Morph.io page for this scraper open in another tab or window - you will need to edit the code on GitHub, and run it to see the results on Morph.io.\n",
    "\n",
    "Now we're ready to start.\n",
    "\n",
    "## Introducing the template code\n",
    "\n",
    "When you create a new scraper on Morph.io, it creates it with some template code as shown below. \n",
    "\n",
    "Each line begins with a hash symbol: `#`. There are two ways that these are most commonly used:\n",
    "\n",
    "* Firstly, as a way of creating **comments** in Python code: any code starting with a `#` does not do anything, so the hash symbol allows you to add comments which are not treated as working code.\n",
    "* Secondly, as a way of *disabling* code - what's called *commenting out* code. Rather than delete an entire line of code, it is easier to add a `#` at the front to turn it 'off' to test what happens, so you can always turn it back 'on' again quickly by removing the `#`.\n",
    "\n",
    "In the template code below generated by Morph.io, the *entire* code is commented out. The idea is that you can **uncomment** the sections you want to use in your own code, saving you time writing scraping code from scratch. We'll come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "# import scraperwiki\n",
    "# import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "# html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries in Morph.io\n",
    "\n",
    "Make sure you are on this file in GitHub, and click the edit button to make some changes.\n",
    "\n",
    "Uncomment the two lines that start with `import` so the code looks like below.\n",
    "\n",
    "These two lines bring in two **libraries** to Morph.io: \n",
    "\n",
    "* Scraperwiki is a library which has useful functions for scraping webpages and storing the results in a database\n",
    "* lxml.html is a library which is useful for *parsing* HTML webpages - i.e. drilling down to particular pieces of information you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "# html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, uncomment the line `html = scraperwiki.scrape(\"http://foo.com\")`. \n",
    "\n",
    "This line is looking at a URL - foo.com - so it's worth checking that site in another window to see what's there.\n",
    "\n",
    "It's in parentheses, which means it's being used as an ingredient in a function - `scrape()`. Specifically, `scraperwiki.scrape()`, which means it's part of the **scraperwiki library**. \n",
    "\n",
    "When using a library it's always useful to check the **documentation** for that library - [here's the documentation for Scraperwiki](https://classic.scraperwiki.com/docs/python/), or at least it's 'Classic' version which was used by Morph.io. There's a link to [where the documentation is now hosted, on GitHub](https://github.com/scraperwiki/code-scraper-in-browser-tool/wiki)\n",
    "\n",
    "The `scrape` function grabs the contents of the given URL and stores it in the new variable `html`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "# root = lxml.html.fromstring(html)\n",
    "# root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **commit** your changes (GitHub's version of saving), and switch back to the scraper in Morph.io. Run the scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of code *converts* the `html` variable into another new variable called `root`, and drills down further into that using something called `cssselect`, which uses **css selectors** to grab very specific pieces of information from the page. We'll talk about this in class but search around for more about those selectors and think how they could be used in scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a template for a Python scraper on morph.io (https://morph.io)\n",
    "# including some code snippets below that you should find helpful\n",
    "\n",
    "import scraperwiki\n",
    "import lxml.html\n",
    "#\n",
    "# # Read in a page\n",
    "html = scraperwiki.scrape(\"http://foo.com\")\n",
    "#\n",
    "# # Find something on the page using css selectors\n",
    "root = lxml.html.fromstring(html)\n",
    "root.cssselect(\"div[align='left']\")\n",
    "#\n",
    "# # Write out to the sqlite database using scraperwiki library\n",
    "# scraperwiki.sqlite.save(unique_keys=['name'], data={\"name\": \"susan\", \"occupation\": \"software developer\"})\n",
    "#\n",
    "# # An arbitrary query against the database\n",
    "# scraperwiki.sql.select(\"* from data where 'name'='peter'\")\n",
    "\n",
    "# You don't have to do things with the ScraperWiki and lxml libraries.\n",
    "# You can use whatever libraries you want: https://morph.io/documentation/python\n",
    "# All that matters is that your final data is written to an SQLite database\n",
    "# called \"data.sqlite\" in the current working directory which has at least a table\n",
    "# called \"data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
